{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce024e6",
   "metadata": {},
   "source": [
    "# EXECUTABLE: LONGITUDINAL PREDICTION IN PRIMARY CARE (PC)\n",
    "\n",
    "The main objective of this notebook is to carry out a time-series forecasting analysis to evaluate and identify diagnostic models that could be useful for predicting others. Previous studies have shown that different types of variables — such as atmospheric, temporal, and mobility variables, among others — can help anticipate increases in population interactions with the healthcare system.\n",
    "\n",
    "These interactions, which mainly take the form of medical visits, are associated with one or more coded diagnoses. This study poses the following working hypothesis:\n",
    "\n",
    "    - Is it possible to accurately predict the number of diagnoses associated with medical visits using past diagnoses as predictor variables?\n",
    "\n",
    "The ultimate goal is to determine which diagnoses have predictive power over others and, consequently, whether they can be used as early indicators of specific pathologies. This knowledge could be especially relevant for:\n",
    "\n",
    "- optimizing healthcare resource planning,\n",
    "- detecting seasonal or emerging disease patterns, and\n",
    "- reinforcing preventive strategies within the scope of primary care.\n",
    "\n",
    "This notebook therefore constitutes a first executive approach to longitudinal prediction based on diagnoses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75569a3d-ef80-4907-932d-2ef30f508777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('__pycache__', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48229bf-e1ab-4b29-88c7-1590a5710115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8a15d-c0f0-407e-a9e4-d352c268ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from darts.models import RNNModel, Theta\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, Theta\n",
    "from darts.metrics import mape, rmse, r2_score\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "from darts.datasets import SunspotsDataset, AirPassengersDataset\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dcebf-6047-4621-96f7-872bdc3f2467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedd60d9",
   "metadata": {},
   "source": [
    "## 1. Data Reading and Processing\n",
    "\n",
    "The algorithm uses temporal diagnostic databases of shape m × n, where m refers to the number of days and n refers to the different diagnoses.\n",
    "\n",
    "From the original dataset, two key transformations are generated:\n",
    "\n",
    "- 1. Smoothed Dataset: Each time series in the database is transformed to become smoother, allowing a clearer view of the underlying trend. The main goal is to eliminate spikes caused by the weekend effect (when there are no visits to primary care and, therefore, no diagnoses are recorded). Additionally, data scaling is applied so that the minimum and maximum values are normalized between 0 and 1. This facilitates the interpretation of performance metrics.\n",
    "\n",
    "- 2. Infection Index Dataset: Some diagnoses refer to infectious diseases, which typically do not behave randomly but instead follow seasonal patterns. The objective of this transformation is to compute an infection risk index, based on the diagnostic activity in the previous 10 days. This aims to capture short-term temporal dependencies associated with infectious outbreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b847108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoother function ---------------------------------\n",
    "#       (revisar --> rolling(funció per fer-ho més fàcil))\n",
    "def smoother(df,window_size):\n",
    "    smoothed = {}\n",
    "    for column in df.columns:\n",
    "        if column != 'visi_data_visita':\n",
    "            column_list = []\n",
    "            for i in range(len(df)):\n",
    "                if i == 0:\n",
    "                    column_list.append(df[column].iloc[0])\n",
    "                elif i < window_size:\n",
    "                    column_list.append(df[column].iloc[:i+1].mean())\n",
    "                else:\n",
    "                    column_list.append(df[column].iloc[i-window_size:i+1].mean())\n",
    "            smoothed[column] = column_list\n",
    "\n",
    "    smoothed = pd.DataFrame.from_dict(smoothed)\n",
    "    smoothed.set_index(df.index, inplace=True)\n",
    "    return smoothed\n",
    "\n",
    "# infection index function -------------------\n",
    "def infection_index_df(df,prior_days):\n",
    "    infection_index = {}\n",
    "    cols = list(df.columns)\n",
    "    for column in cols:\n",
    "        column_list = []\n",
    "        for i in range(len(df)):\n",
    "            if i < prior_days:\n",
    "                column_list.append(float('nan'))\n",
    "            else:\n",
    "                if sum(df[column].iloc[(i-prior_days):(i-1)]) > 0:\n",
    "                    inf_idx = df[column].iloc[i] / sum(df[column].iloc[(i-prior_days):(i-1)])\n",
    "                else:\n",
    "                    inf_idx = float('nan')\n",
    "                column_list.append(inf_idx)\n",
    "        infection_index[column] = column_list\n",
    "    infection_index = pd.DataFrame.from_dict(infection_index)\n",
    "    infection_index.set_index(df.index, inplace=True)\n",
    "    return infection_index\n",
    "\n",
    "\n",
    "# plot example 10 diags -------------------------\n",
    "def plot_example(df, title):\n",
    "    \"\"\"\n",
    "    Randomly selects and plots 10 time series from a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Time-indexed DataFrame with multiple columns (e.g. diagnostics).\n",
    "    - title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Randomly sample 10 columns\n",
    "    sampled_cols = np.random.choice(df.columns, size=10, replace=False)\n",
    "    dff = df[sampled_cols].copy()\n",
    "\n",
    "    # Add date column\n",
    "    dff[\"date\"] = dff.index\n",
    "\n",
    "    # Melt and plot\n",
    "    sns.set(rc={'figure.figsize': (20, 8)})\n",
    "    sns.lineplot(\n",
    "        data=dff.melt(id_vars=['date']),\n",
    "        x='date', y='value', hue='variable'\n",
    "    ).set(title=title)\n",
    "\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# plot example 10 diags -------------------------\n",
    "def ploter(df,title,n,code):\n",
    "    # PLOT RAW DATA (example 10 diags)\n",
    "    cols = list(df.columns)[0:n]\n",
    "    cols.append(code)\n",
    "    dff = df[cols]\n",
    "    dff[\"date\"] = dff.index\n",
    "    sns.set(rc={'figure.figsize':(20,8)})\n",
    "    sns.lineplot(data=dff.replace('nan', float('nan')).melt(id_vars=['date']),x='date', y='value', hue='variable').set(title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692047f",
   "metadata": {},
   "source": [
    "### 1.1 Databases plots (10 example diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76397f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA --------------------------------------------------------------\n",
    "df = pd.read_csv('synthetic_timeseries.csv', index_col=0)\n",
    "df.index = pd.date_range(start=\"2010-01-01\", periods=len(df), freq=\"D\")\n",
    "df = df.clip(lower=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(df,\"RAW DATA (example 10 ts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298add56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOOTHED DATA --------------------------------------------------------\n",
    "window_size = 14\n",
    "smoothed = smoother(df, window_size)\n",
    "plot_example(smoothed,\"SMOOTHED (example 10 ts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc010cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_scaled = (smoothed-smoothed.min())/(smoothed.max()-smoothed.min()) # normalize df\n",
    "plot_example(smoothed_scaled,\"SMOOTHED + SCALED DATA (example 10 ts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = smoothed_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbdd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e0c059",
   "metadata": {},
   "source": [
    "## 2. Cross-Correlation Analysis\n",
    "\n",
    "The objective at this stage is to identify which diagnoses behave similarly, and therefore may provide redundant information to the predictive models. The ultimate goal is to define models that are as simple and parsimonious as possible — that is, models that use the smallest number of variables while still accurately predicting the target diagnosis.\n",
    "\n",
    "At this point, we define the target variable to be predicted based on the following parameters:\n",
    "\n",
    "- 1. code: The code corresponding to the diagnosis we aim to predict (i.e., the dependent variable).\n",
    "- 2. correlation_threshold: The maximum allowed correlation between variables. Variables with a correlation above 90% with any other are considered redundant and excluded from the model.\n",
    "\n",
    "This step ensures that the input features used for training are informative but not collinear, thus improving both model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenir variables correlacionades ------------------------------------------\n",
    "# WARNING = No utilitzar scaled data\n",
    "def get_top_correlations_blog(df, threshold=0.90):\n",
    "    orig_corr = df.corr()\n",
    "    c = orig_corr.abs()\n",
    "    so = c.unstack()\n",
    "\n",
    "    i = 0\n",
    "    pairs = set()\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for index, value in so.sort_values(ascending=False).items():  # <-- FIXED HERE\n",
    "        # Exclude duplicates and self-correlations\n",
    "        if (\n",
    "            value > threshold\n",
    "            and index[0] != index[1]\n",
    "            and (index[0], index[1]) not in pairs\n",
    "            and (index[1], index[0]) not in pairs\n",
    "        ):\n",
    "            result.loc[i, ['Variable 1', 'Variable 2', 'Correlation Coefficient']] = [\n",
    "                index[0],\n",
    "                index[1],\n",
    "                orig_corr.loc[index[0], index[1]],  # cleaner tuple access\n",
    "            ]\n",
    "            pairs.add((index[0], index[1]))\n",
    "            i += 1\n",
    "\n",
    "    return result.reset_index(drop=True).set_index(['Variable 1', 'Variable 2'])\n",
    "\n",
    "# Reduir el nombre de variables correlacionades ------------------------------\n",
    "def compute_vif(considered_features, df):\n",
    "    \n",
    "    X = df[considered_features]\n",
    "    # the calculation of variance inflation requires a constant\n",
    "    X['intercept'] = 1\n",
    "    \n",
    "    # create dataframe to store vif values\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"Variable\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif = vif[vif['Variable']!='intercept']\n",
    "    return vif\n",
    "\n",
    "# filter variables based on VIF threshold -------------------------------\n",
    "def filter_VIF(vif,dataframe,iterations_max,VIF_threshold):\n",
    "    iterations = 0\n",
    "    while vif['VIF'].iloc[0] > VIF_threshold and iterations < iterations_max:\n",
    "        redundant_vars.pop(0)\n",
    "        vif = compute_vif(redundant_vars, dataframe).sort_values('VIF', ascending=False)\n",
    "        \n",
    "        if iterations % 5 == 0:\n",
    "            print(\"... iteration ==> \"+str(iterations)+\" max VIF ==> \"+str(vif.iloc[vif['VIF'].idxmax()][\"VIF\"]))\n",
    "        \n",
    "        iterations += 1\n",
    "    \n",
    "    print(\">>> VIF variables surpassing thresholds ....\")\n",
    "    display(vif)\n",
    "    \n",
    "    return redundant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc800fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAG\n",
    "code = 'timeseries_350'\n",
    "\n",
    "# CORR\n",
    "correlation_threshold = 0.90\n",
    "\n",
    "# VIF\n",
    "iterations_max = 400\n",
    "iterations = 0\n",
    "VIF_threshold = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f91950",
   "metadata": {},
   "source": [
    "1. Computem tots els coeficients de correlació entre les variables d'estudi i ens quedem amb aquells que superen un llindar de 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the smoothed dataframe\n",
    "dataframe = smoothed.copy()\n",
    "\n",
    "# Compute top correlations above a threshold\n",
    "df_correlations = get_top_correlations_blog(dataframe, threshold=correlation_threshold)\n",
    "display(df_correlations)\n",
    "\n",
    "# Extract correlated variable names from both levels of the MultiIndex\n",
    "correlated_vars = list(set(df_correlations.index.get_level_values(0).tolist() + \n",
    "                           df_correlations.index.get_level_values(1).tolist()))\n",
    "\n",
    "# Compute variables not involved in any strong correlation\n",
    "non_correlated_vars = list(set(dataframe.columns) - set(correlated_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7b909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d229ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16c0c86e",
   "metadata": {},
   "source": [
    "2. Computem els valors VIF de les variables i reduïm les variables amb alta correlació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables que són subjecte de ser eliminades\n",
    "redundant_vars = correlated_vars\n",
    "\n",
    "# computar el vif de cadascuna\n",
    "vif = compute_vif(redundant_vars, dataframe).sort_values('VIF', ascending=False)\n",
    "display(vif)\n",
    "\n",
    "# filtrar el vif\n",
    "redundant_vars = filter_VIF(vif,dataframe,iterations_max, VIF_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5606e",
   "metadata": {},
   "source": [
    "3. Exemple de reducció de la matriu de correlacions (només funciona quan el nombre de variables és inferior a n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "if dataframe.shape[1] < n:\n",
    "    # ABANS -----------------------------------------------------\n",
    "    # set figure size \n",
    "    plt.figure(figsize=(20,12))\n",
    "    # Generate a mask to onlyshow the bottom triangle\n",
    "    mask = np.triu(np.ones_like(dataframe.corr(), dtype=bool))\n",
    "    # generate heatmap\n",
    "    sns.heatmap(dataframe.corr(), annot=True, mask=mask, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Coefficient Of Predictors')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # DESPRÉS -------------------------------------------------\n",
    "    interesting_vars = redundant_vars+non_correlated_vars\n",
    "    # set figure size\n",
    "    plt.figure(figsize=(20,12))\n",
    "    # Generate a mask to onlyshow the bottom triangle\n",
    "    mask = np.triu(np.ones_like(dataframe[interesting_vars].corr(), dtype=bool))\n",
    "    # generate heatmap\n",
    "    sns.heatmap(dataframe[interesting_vars].corr(), annot=True, mask=mask, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Coefficient Of Predictors')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837454bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of dataframe with interesting vars\n",
    "print(\">>> Initial number of variables in dataframe ... %s\" % dataframe.shape[1])\n",
    "interesting_vars = redundant_vars+non_correlated_vars \n",
    "dataframe = dataframe[interesting_vars]\n",
    "dataframe[code] = smoothed[code] # tornem a addherir la variable a predir\n",
    "print(\">>> Final number of variables in dataframe ... %s\" % dataframe.shape[1])\n",
    "non_scaled_df = dataframe.copy()\n",
    "\n",
    "print(\"... Scaling dataframe ===>\")\n",
    "dataframe = (dataframe-dataframe.min())/(dataframe.max()-dataframe.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fa11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-scaled data\n",
    "ploter(non_scaled_df,\"SMOOTHED FILTERED DATA\", 20, code)\n",
    "non_scaled_df = non_scaled_df.drop([code], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e00f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data \n",
    "ploter(dataframe,\"SMOOTHED FILTERED DATA (SCALED)\", 20, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266641a",
   "metadata": {},
   "source": [
    "## 3. MODEL CONSTRUCTION\n",
    "\n",
    "Next, we will develop a predictive model for the selected diagnosis code. The modeling process consists of two main phases:\n",
    "\n",
    "- 1. Construction of the optimal model M at time t: In this phase, the goal is to build and train the best-performing model using all available information up to time t. This model will serve as a baseline for subsequent comparisons.\n",
    "- 2. Testing of model M across different time lags, and construction of new models at times t - i: Here, we assess the robustness and generalizability of model M by applying it retrospectively to earlier time points (i.e., t - i) and comparing its performance with newly trained models specific to those earlier intervals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculation(models,y_train,y_test,Y_PREDICTIONS_train,Y_PREDICTIONS_test,params):\n",
    "\n",
    "    F1 = []\n",
    "    F2 = []\n",
    "    p_values1 = []\n",
    "    p_values2 = []\n",
    "    n_vars = []\n",
    "    MAPE_ts = []\n",
    "    MAPE_tr = []\n",
    "    RMSE_tr = []\n",
    "    RMSE_ts = []\n",
    "    pars = []\n",
    "\n",
    "\n",
    "    for i in range(len(models) - 1):\n",
    "        n = int(models[i].summary().tables[0].data[5][1])\n",
    "\n",
    "        k1 = int(models[i].summary().tables[0].data[7][1])\n",
    "        k2 = int(models[i+1].summary().tables[0].data[7][1])\n",
    "\n",
    "        rss1 = models[i].ssr\n",
    "        rss2 = models[i+1].ssr\n",
    "\n",
    "        # f stat internet\n",
    "        Fstat = ((rss1-rss2)/(k2-k1))/(rss2/(n-k2))\n",
    "        p_value1 = 1-scipy.stats.f.cdf(Fstat, k2-k1, n-k2-1)\n",
    "\n",
    "        #f stat paper\n",
    "        sse2 = np.sum((Y_PREDICTIONS_train[i+1].values - y_train)**2)\n",
    "        ssr1 = np.sum((Y_PREDICTIONS_train[i].values - y_train.mean())**2)\n",
    "        ssr2 = np.sum((Y_PREDICTIONS_train[i+1].values - y_train.mean())**2)\n",
    "\n",
    "        Fstat2 = ((ssr2-ssr1)/(k2-k1))/(sse2/(n-k2-1))\n",
    "        p_value2 = 1-scipy.stats.f.cdf(Fstat2, k2-k1, n-k2-1)\n",
    "\n",
    "        # mape\n",
    "        mape_tr = mean_absolute_error(y_train, Y_PREDICTIONS_train[i])*100\n",
    "        mape_ts = mean_absolute_error(y_test, Y_PREDICTIONS_test[i])*100\n",
    "\n",
    "        #rmse\n",
    "        rmse_tr = math.sqrt(mean_squared_error(y_train, Y_PREDICTIONS_train[i]))\n",
    "        rmse_ts = math.sqrt(mean_squared_error(y_test, Y_PREDICTIONS_test[i]))\n",
    "\n",
    "        F1.append(Fstat)\n",
    "        F2.append(Fstat2)\n",
    "        p_values1.append(p_value1)\n",
    "        p_values2.append(p_value2)\n",
    "        n_vars.append(i)\n",
    "        MAPE_ts.append(mape_ts)\n",
    "        MAPE_tr.append(mape_tr)\n",
    "        RMSE_ts.append(rmse_ts)\n",
    "        RMSE_tr.append(rmse_tr)\n",
    "        pars.append(params[i])\n",
    "    \n",
    "    res = pd.DataFrame()\n",
    "    res['number_of_variables'] = n_vars\n",
    "    res[\"F1 (complex vs. intercept)\"] = F1\n",
    "    res['p-value (F1)'] = p_values1\n",
    "    res[\"F2 (Complex vs. anterior)\"] = F2\n",
    "    res['p-value (F2)'] = p_values2\n",
    "    res['MAPE train'] = MAPE_tr\n",
    "    res['MAPE test'] = MAPE_ts\n",
    "    res['RMSE train'] = RMSE_tr\n",
    "    res['RMSE test'] = RMSE_ts\n",
    "    res['predictors'] = pars\n",
    "    \n",
    "    return res\n",
    "\n",
    "def models_training(model_vars,code, corr,max_iters):\n",
    "    \n",
    "    print(\">>> Training models for predicting variable ... \"+code)\n",
    "    \n",
    "    Y_PREDICTIONS_test = []\n",
    "    Y_PREDICTIONS_train = []\n",
    "    predictors = []\n",
    "    models = []\n",
    "    \n",
    "    corr = corr[0:max_iters]\n",
    "\n",
    "    # train test split ------------------------------------------------\n",
    "    split_index = round(len(model_vars)*0.8)\n",
    "    split_date = model_vars.index[split_index]\n",
    "\n",
    "    df_train = model_vars.loc[model_vars.index <= split_date].copy()\n",
    "    df_test = model_vars.loc[model_vars.index > split_date].copy()\n",
    "\n",
    "    y_train = df_train[code].values\n",
    "    y_test = df_test[code].values\n",
    "\n",
    "    for i in range(1,len(corr)):\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(\">>> Iteration %s\" % str(i))\n",
    "        \n",
    "\n",
    "        X_train = df_train\n",
    "        X_train = X_train[corr.index[0:i].tolist()]\n",
    "\n",
    "        X_test = df_test\n",
    "        X_test = X_test[corr.index[0:i].tolist()]\n",
    "\n",
    "        ols_model = sm.OLS(y_train,X_train)\n",
    "        ols_results = ols_model.fit()\n",
    "        models.append(ols_results)\n",
    "        predictors.append(\",\".join(pd.DataFrame(ols_results.params).index.tolist()))\n",
    "\n",
    "        y_pred_train = ols_results.predict(X_train)\n",
    "        y_pred_test = ols_results.predict(X_test)\n",
    "        Y_PREDICTIONS_test.append(y_pred_test)\n",
    "        Y_PREDICTIONS_train.append(y_pred_train)\n",
    "    \n",
    "\n",
    "    print(\">>> ALL MODELS -----------------------------------------\")\n",
    "    \n",
    "    # plot all models\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(code+' prediction')\n",
    "    lines = [] \n",
    "    for i in range(len(corr)-1):\n",
    "        lines += plt.plot(df_test.index, Y_PREDICTIONS_test[i], label='%s iteration' % i)\n",
    "    lines += plt.plot(df_test.index, y_test, 'go-', label='Actual Cov_19 diagnoses')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('n diagnoses')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # metrics \n",
    "    df = metrics_calculation(models,y_train,y_test,Y_PREDICTIONS_train,Y_PREDICTIONS_test,predictors)\n",
    "\n",
    "    sns.lineplot(data=df[['number_of_variables',\"F1 (complex vs. intercept)\",\"F2 (Complex vs. anterior)\"]].melt(id_vars=['number_of_variables']),x='number_of_variables',y='value',hue=\"variable\")\n",
    "    plt.show()\n",
    "    sns.lineplot(data=df[['number_of_variables',\"p-value (F1)\",\"p-value (F2)\"]].melt(id_vars=['number_of_variables']),x='number_of_variables',y='value',hue=\"variable\")\n",
    "    plt.show()\n",
    "\n",
    "    # models +++ interessants -------------------------\n",
    "    interesting_models = [] # guardats\n",
    "    n_int = 3               # fins a 3 models que passin el pval\n",
    "    pval = 0.1              # pval\n",
    "    c = 0\n",
    "    for i in range(len(df)):\n",
    "        if df['p-value (F1)'][i] >= pval:\n",
    "            c += 1\n",
    "            if c <= 3:\n",
    "                interesting_models.append(i)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    df = df[0:max(interesting_models)+1]\n",
    "    print(\">>> BEST INTERESTING MODELS -----------------------------------------\")\n",
    "    display(df)\n",
    "    \n",
    "    # plot interesting\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(code+' prediction')\n",
    "    lines = [] \n",
    "    for i in range(len(interesting_models)):\n",
    "        lines += plt.plot(df_test.index, Y_PREDICTIONS_test[interesting_models[i]], label='%s iteration' % str(interesting_models[i]))\n",
    "\n",
    "    lines += plt.plot(df_test.index, y_test, 'go-', label='Actual diagnoses')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('n diagnoses')\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    plt.legend(lines, labels) \n",
    "    plt.show()\n",
    "    \n",
    "    #### BEST ABSOLUTE MODEL\n",
    "\n",
    "    print(\">>> ABSOLUTE BEST MODEL -----------------------------------------\")\n",
    "    print(models[min(interesting_models)].summary())    \n",
    "    \n",
    "    df['BEST_MODEL'] = \"NO\"\n",
    "    df.loc[min(interesting_models),\"BEST_MODEL\"] = \"YES\"\n",
    "    \n",
    "    # plot interesting\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(code+' prediction')\n",
    "    lines = [] \n",
    "    lines += plt.plot(df_test.index, Y_PREDICTIONS_test[min(interesting_models)], label='%s iteration' % str(min(interesting_models)))\n",
    "    lines += plt.plot(df_test.index, y_test, 'go-', label='Actual diagnoses')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('n diagnoses')\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    plt.legend(lines, labels) \n",
    "    plt.show()\n",
    "    \n",
    "    display(df[0:min(interesting_models)+1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bed9c",
   "metadata": {},
   "source": [
    "### A. Real-time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c53af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iters = 601\n",
    "\n",
    "# sort absolute correations between variables and cov19\n",
    "smot_corr = non_scaled_df.corrwith(smoothed[code]).sort_values(ascending=False, key=abs)\n",
    "\n",
    "df = models_training(dataframe,code,smot_corr,max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bd189",
   "metadata": {},
   "source": [
    "### B. Lagged prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 75\n",
    "max_lag = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86118a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smot_corr = non_scaled_df.corrwith(smoothed[code]).sort_values(ascending=False, key=abs)\n",
    "\n",
    "# A. initial model (on-time prediction) ----------------\n",
    "df_init = models_training(dataframe,code,smot_corr,max_iters)\n",
    "BEST = df_init[df_init[\"BEST_MODEL\"] == \"YES\"]\n",
    "BEST[\"LAG\"] = 0 \n",
    "\n",
    "for i in range(1,max_lag+1):\n",
    "    dat_lag = dataframe.copy()\n",
    "    # add lag\n",
    "    dat_lag[code] = dat_lag[code].shift(-i)\n",
    "    dat_lag = dat_lag.dropna(subset=[code])\n",
    "    \n",
    "    # OBTENIR EL MILLOR MODEL\n",
    "    df_init = models_training(dat_lag,code,smot_corr,max_iters)\n",
    "    best = df_init[df_init[\"BEST_MODEL\"] == \"YES\"]\n",
    "    best[\"LAG\"] = i\n",
    "    BEST = pd.concat([BEST,best]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST.to_excel(\"BEST_features_NOSMOOTH_timeseries350.xlsx\", index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27674b",
   "metadata": {},
   "source": [
    "### EVALUATION METRICS ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397133c",
   "metadata": {},
   "source": [
    "#### A. MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = BEST[[\"MAPE train\", \"MAPE test\", \"LAG\"]]\n",
    "sns.set(rc={'figure.figsize':(20,8)})\n",
    "sns.lineplot(data=dff.replace('nan', float('nan')).melt(id_vars=['LAG']),x='LAG', y='value', hue='variable').set(title=\"MAPE ANALYSISIS IN LAGGED PREDICTION\")\n",
    "plt.axvline(7, 0,BEST[[\"MAPE train\", \"MAPE test\"]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(14, 0,BEST[[\"MAPE train\", \"MAPE test\"]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(21, 0,BEST[[\"MAPE train\", \"MAPE test\"]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(28, 0,BEST[[\"MAPE train\", \"MAPE test\"]].max(0).max(0), color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73114588",
   "metadata": {},
   "source": [
    "#### B. RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = BEST[[\"RMSE train\", \"RMSE test\", \"LAG\"]]\n",
    "sns.set(rc={'figure.figsize':(20,8)})\n",
    "sns.lineplot(data=dff.replace('nan', float('nan')).melt(id_vars=['LAG']),x='LAG', y='value', hue='variable').set(title=\"RMSE ANALYSISIS IN LAGGED PREDICTION\")\n",
    "plt.axvline(7, 0,BEST[[\"RMSE train\", \"RMSE test\", \"LAG\"]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(14, 0,BEST[[\"RMSE train\", \"RMSE test\", \"LAG\"]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(21, 0,BEST[[\"RMSE train\", \"RMSE test\", \"LAG\"]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(28, 0,BEST[[\"RMSE train\", \"RMSE test\", \"LAG\"]].max(0).max(0), color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b063a",
   "metadata": {},
   "source": [
    "#### C. F vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = BEST[[\"F1 (complex vs. intercept)\", \"p-value (F1)\", \"F2 (Complex vs. anterior)\", \"p-value (F2)\", \"LAG\"]]\n",
    "sns.set(rc={'figure.figsize':(20,8)})\n",
    "sns.lineplot(data=dff.replace('nan', float('nan')).melt(id_vars=['LAG']),x='LAG', y='value', hue='variable').set(title=\"F vals ANALYSISIS IN LAGGED PREDICTION\")\n",
    "plt.axvline(7, 0,BEST[[\"F1 (complex vs. intercept)\", \"p-value (F1)\", \"F2 (Complex vs. anterior)\", \"p-value (F2)\",]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(14, 0,BEST[[\"F1 (complex vs. intercept)\", \"p-value (F1)\", \"F2 (Complex vs. anterior)\", \"p-value (F2)\",]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(21, 0,BEST[[\"F1 (complex vs. intercept)\", \"p-value (F1)\", \"F2 (Complex vs. anterior)\", \"p-value (F2)\",]].max(0).max(0), color=\"red\")\n",
    "plt.axvline(28, 0,BEST[[\"F1 (complex vs. intercept)\", \"p-value (F1)\", \"F2 (Complex vs. anterior)\", \"p-value (F2)\",]].max(0).max(0), color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfa335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff338f8",
   "metadata": {},
   "source": [
    "## C. Prediction lagged first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5432bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_PREDICTIONS_test = []  # training results\n",
    "Y_PREDICTIONS_train = [] # testing results\n",
    "no_lag_predictors = True\n",
    "\n",
    "for lagging in range(0,max_lag):\n",
    "    \n",
    "    # predictors del millor model per a lag == lagging (0,30) o inicial\n",
    "        # varien en funció del model\n",
    "    if no_lag_predictors == True:\n",
    "        predictors = BEST.loc[0,\"predictors\"].split(\",\")\n",
    "    else:\n",
    "        predictors = BEST.loc[lagging,\"predictors\"].split(\",\")\n",
    "    \n",
    "    # copia de la bbbdd\n",
    "    df = dataframe.copy()\n",
    "    split_index = round(len(df)*0.8) \n",
    "    df = df[[code]+predictors]\n",
    "    \n",
    "    # Realitzem una copia lagged de la bbdd\n",
    "    if lagging > 0:\n",
    "        df[code] = df[code].shift(-lagging)\n",
    "        df = df.dropna(subset=[code])\n",
    "    print(\"TRAINING ROWS post lagging % s\" % str(df.shape))\n",
    "\n",
    "    \n",
    "    #train_test_split\n",
    "    split_date = df.index[split_index-lagging]\n",
    "    df_train = df.loc[df.index <= split_date].copy()\n",
    "    df_test = df.loc[df.index > split_date].copy()\n",
    "    y_train = df_train[code].values\n",
    "    y_test = df_test[code].values\n",
    "    X_train = df_train\n",
    "    X_train = X_train[predictors]\n",
    "    X_test = df_test\n",
    "    X_test = X_test[predictors]\n",
    "\n",
    "    # model training\n",
    "    ols_model = sm.OLS(y_train,X_train)\n",
    "    ols_results = ols_model.fit()\n",
    "\n",
    "    #populate vars\n",
    "    y_pred_train = ols_results.predict(X_train)\n",
    "    y_pred_test = ols_results.predict(X_test)\n",
    "    Y_PREDICTIONS_test.append(y_pred_test)\n",
    "    Y_PREDICTIONS_train.append(y_pred_train)\n",
    "\n",
    "# plot all models\n",
    "fig = plt.figure()\n",
    "fig.suptitle(code+' prediction')\n",
    "lines = [] \n",
    "\n",
    "for i in range(len(Y_PREDICTIONS_test)):#-1):\n",
    "    lines += plt.plot(df_test.index, Y_PREDICTIONS_test[i], label='%s day lagging' % str(i+1))\n",
    "lines += plt.plot(df_test.index, y_test, 'go-', label='Actual %s diagnoses' % code)\n",
    "labels = [l.get_label() for l in lines]\n",
    "plt.legend(lines, labels) \n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('n diagnoses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdfe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55615474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9327e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81fe02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae42d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918b462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d29519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779c58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c461b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e42be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b68d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c418b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde07bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452d086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e2d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab6c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affebf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed5f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92611da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80915f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506907d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebaaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db53d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26aa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8f387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c5595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0777b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b1d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74be6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf2f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee66d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742d614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10857471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028a792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea626c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e8943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173d85a6-11a7-45a0-902e-9fdac0a8c308",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
